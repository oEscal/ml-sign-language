\section{\textit{Multinomial logistic regression}}
Decidimos fazer um estudo com este modelo para esclarecer a eficácia do mesmo, que previamente achava-mos que ia ser baixa. Como se pode comprovar de seguida os resultados foram surpreendentes.
\textit{Multinomial logistic regression}, de entre os três estudados, é o algoritmo mais simples em termos matemáticos e de  implementação. Este algoritmo consiste na generalização de \textit{logistic regression} para problemas de multi-classes,assemelhando-se a uma função de pesos lineares. Posto isto é um modelo que é usado para calcular probabilidades de possíveis diferentes \textit{outcomes}, tendo como valores de entrada um conjunto de variáveis independentes que podem ser de natureza real, binária ou categorial,etc.

\subsection{Valores \textit{defaults} dos hiperparâmetros}

\begin{itemize}
    \item \textit{C} = 1
    \item numero máximo de iterações = 1000
\end{itemize}

\subsection{Estudo da variação de \textit{C}}
Sabendo que este parâmetro se trata do inverso de \\lambda,ou seja é o parâmetro responsável pela regularização dos pesos para evitar fenómenos de \textit{overfit},foram definidos os seguintes valores para a realização do estudo: $\textit{C} \in \{0.001, 0.002, 0.01, 0.02, 0.1, 0.2, 1, 5, 10, 50, 100, 500, 1000\}$.
No gráfico \ref{fig:model_lr_C} pode se visualizar a curva de validação para vários valores de \textit{C} e no gráfico \ref{fig:time_operations_lr_C} o tempo das operações de \textit{fit} e \textit{predict} dos modelos para vários valores de \textit{C}.
Como se pode visualizar, para valores de \textit{C} baixos o modelo não consegue criar um função de decisão suficientemente complexa para lidar com a forma dos dados e por isso ocorre o fenómeno de \textit{underfit}, que é justificável por ambos os erros serem elevados. Este erro começa a diminuir com o aumento de C, chegando a valores muito próximos de 0 com \textit{C} \\geqslant 1.
De todos os algoritmos estudados este acabou por ser o algoritmo com tempos de execução mais rápidos,o que era de esperar.

\begin{figure}[htp]
\centering
\includegraphics[width=3in]{figures/validation_curve_lr_C.png}
\caption{Curva de validação para vários valores de \textit{C}}
\label{fig:model_lr_C}
\end{figure}

\begin{figure}[htp]
\centering
\includegraphics[width=3in]{figures/time_per_parameter_lr_C.png}
\caption{Tempo das operações de \textit{Fit} e \textit{Predict} dos modelos para vários valores de \textit{C}}
\label{fig:time_operations_lr_C}
\end{figure}

\subsection{Estudo da variação de \textit{numero máximo de iterações}(\textit{iter}}

Este parâmetro define o numero máximo de iterações que os \textit{solvers} têm para conseguir convergir, tendo sido definido os seguintes valores: $\textit{iter} \in \{200, 500, 1000, 2000\}$. 

No gráfico \ref{fig:model_lr_iter} será apresentado a curva de validação para vários valores de iterações e no gráfico \ref{fig: time_operations_lr_iter} os tempos de execução dos processos de \textit{fit} e \textit{predict}.
Como era de esperar para um numero mais elevado de iterações a \textit{accuracy} do algoritmo aumenta pois este tem mais iterações para chegar ao ponto de convergência, adversamente o tempo de execução do processo de \textit{fit} aumenta.


\begin{figure}[htp]
\centering
\includegraphics[width=3in]{figures/validation_curve_lr_iter.png}
\caption{Curva de validação para vários valores de \textit{iter}}
\label{fig:model_lr_iter}
\end{figure}


\begin{figure}[htp]
\centering
\includegraphics[width=3in]{figures/time_per_parameter_lr_iter.png}
\caption{Curva de validação para vários valores de \textit{iter}}
\label{fig: time_operations_lr_iter}
\end{figure}



\subsection{Conclusões}
Com o estudo deste algoritmo foi possível concluir, que para o \textit{dataset} em questão apesar de se tratar de imagens, este algoritmo simples conseguiu obter resultados impressionantes superando as nossas expectativas.
Aqui o parâmetro \textit{C} foi decisivo no resultado do modelo na medida em que ajusta os pesos da \textit{decision boundary} possibilitando a criação de uma função mais complexa ou mais rígida.
Neste caso para valores demasiados pequeno de \textit{C} o modelo acaba por não conseguir generalizar os dados pois a função é demasiado simples, causando \textit{underfit} , como é visível no gráfico \ref{fig:model_lr_C}. Para valores maiores e razoáveis de C a \textit{accuracy} do modelo aumenta.


No final deste processo os melhores parâmetros foram os seguintes:
\begin{itemize}
    \item \textit{C} = 500
    \item \textit{maxIter} = 2000
\end{itemize}

Após o último processo de treino com os melhores parâmetros, obtivemos as seguintes tabelas: métricas de performance \ref{tab: lr_perfomance} e matriz de confusão \ref{tab: lr_confusion_matrix}.


\begin{table}[!htp]
\caption{Métricas de perfomance para \textit{C} = 500 e \textit{maxIter} = 2000}
\begin{center}
\begin{tabular}{l c c c c}
Class & Accuracy & Recall & Precision & F1 Score\\ \hline
0 & 1.0 & 1.0 & 1.0 & 1.0\\
1 & 1.0 & 1.0 & 1.0 & 1.0\\
2 & 1.0 & 1.0 & 1.0 & 1.0\\
3 & 1.0 & 1.0 & 1.0 & 1.0\\
4 & 1.0 & 1.0 & 1.0 & 1.0\\
5 & 1.0 & 1.0 & 1.0 & 1.0\\
6 & 1.0 & 1.0 & 0.997 & 0.998\\
7 & 0.997 & 0.997 & 1.0 & 0.998\\
8 & 1.0 & 1.0 & 1.0 & 1.0\\
10 & 1.0 & 1.0 & 1.0 & 1.0\\
11 & 1.0 & 1.0 & 1.0 & 1.0\\
12 & 1.0 & 1.0 & 1.0 & 1.0\\
13 & 1.0 & 1.0 & 1.0 & 1.0\\
14 & 1.0 & 1.0 & 1.0 & 1.0\\
15 & 1.0 & 1.0 & 1.0 & 1.0\\
16 & 1.0 & 1.0 & 1.0 & 1.0\\
17 & 1.0 & 1.0 & 1.0 & 1.0\\
18 & 1.0 & 1.0 & 1.0 & 1.0\\
19 & 1.0 & 1.0 & 1.0 & 1.0\\
20 & 1.0 & 1.0 & 1.0 & 1.0\\
21 & 1.0 & 1.0 & 1.0 & 1.0\\
22 & 1.0 & 1.0 & 1.0 & 1.0\\
23 & 1.0 & 1.0 & 1.0 & 1.0\\
24 & 1.0 & 1.0 & 1.0 & 1.0\\
\hline
Macro Average & 1.0 & 1.0 & 1.0 & 1.0\\
\end{tabular}

\label{tab: lr_perfomance}
\end{center}
\end{table}


\begin{table*}[!htp]
\caption{Matriz de confusão para \textit{C} = 500 e \textit{maxIter} = 2000 }
\begin{center}
\setlength{\tabcolsep}{0.5em}
\begin{tabular}{l l|c c c c c c c c c c c c c c c c c c c c c c c c }
{} & {} & \multicolumn{24}{c}{Actual Class}\\
{} & Class&0&1&2&3&4&5&6&7&8&10&11&12&13&14&15&16&17&18&19&20&21&22&23&24\\
\hline
\multirow{24}{*}{\rotatebox[origin=c]{90}{ Predicted Class}}&0&301&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\
&1&0&291&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\
&2&0&0&281&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\
&3&0&0&0&282&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\
&4&0&0&0&0&278&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\
&5&0&0&0&0&0&296&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\
&6&0&0&0&0&0&0&305&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\
&7&0&0&0&0&0&0&1&313&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\
&8&0&0&0&0&0&0&0&0&288&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\
&10&0&0&0&0&0&0&0&0&0&250&0&0&0&0&0&0&0&0&0&0&0&0&0&0\\
&11&0&0&0&0&0&0&0&0&0&0&274&0&0&0&0&0&0&0&0&0&0&0&0&0\\
&12&0&0&0&0&0&0&0&0&0&0&0&311&0&0&0&0&0&0&0&0&0&0&0&0\\
&13&0&0&0&0&0&0&0&0&0&0&0&0&281&0&0&0&0&0&0&0&0&0&0&0\\
&14&0&0&0&0&0&0&0&0&0&0&0&0&0&270&0&0&0&0&0&0&0&0&0&0\\
&15&0&0&0&0&0&0&0&0&0&0&0&0&0&0&272&0&0&0&0&0&0&0&0&0\\
&16&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&290&0&0&0&0&0&0&0&0\\
&17&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&289&0&0&0&0&0&0&0\\
&18&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&294&0&0&0&0&0&0\\
&19&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&309&0&0&0&0&0\\
&20&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&258&0&0&0&0\\
&21&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&287&0&0&0\\
&22&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&306&0&0\\
&23&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&291&0\\
&24&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&0&307\\
\end{tabular}
\label{tab: lr_confusion_matrix}
\end{center}
\end{table*}
