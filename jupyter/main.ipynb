{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%capture\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c68e0d535ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3c68e0d535ae>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0mdegrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from enum import Enum, unique\n",
    "from os import listdir\n",
    "import logging\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import classification_report, mean_squared_error, precision_score, confusion_matrix, accuracy_score\n",
    "from abc import ABCMeta\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegressionSKlearn\n",
    "\n",
    "logger = logging.getLogger(\"classifiers\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "from numbers import Number\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import rescale\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from sklearn.base import is_classifier, clone\n",
    "from sklearn.metrics import check_scoring\n",
    "from sklearn.model_selection import learning_curve, check_cv\n",
    "from sklearn.model_selection._validation import _fit_and_score\n",
    "from sklearn.utils import indexable, Parallel, delayed\n",
    "\n",
    "\n",
    "def read_file(path_file: str, shuffle=False) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"Function to read datafile and returns a tuple with the following format: (X, y). X represents all the features\n",
    "        and y represents all the outputs for each data example\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path_file, header=None, skiprows=1).values\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "    return data[:, 0:], data[:, 0].reshape(data.shape[0], 1)\n",
    "\n",
    "\n",
    "def represent_data_graphically(data: np.ndarray, file_save: str, rows: int = 10, cols: int = 10):\n",
    "    data_image_size = int(math.sqrt(len(data[0, :])))\n",
    "    data_len = len(data)\n",
    "\n",
    "    fig, axis = plt.subplots(rows, cols, figsize=(data_image_size, data_image_size))\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            example_id = np.random.randint(data_len)\n",
    "            axis[row, col].imshow(data[example_id, :].reshape(data_image_size, data_image_size, order=\"F\"))\n",
    "    plt.savefig(file_save)\n",
    "\n",
    "\n",
    "def sigmoid(z: Union[Number, np.ndarray]) -> np.ndarray:\n",
    "    \"\"\"return the sigmoid of z\n",
    "    \"\"\"\n",
    "    return .5 * (1 + np.tanh(.5 * z))\n",
    "\n",
    "\n",
    "# Computes the gradient of sigmoid function\n",
    "def sigmoid_gradient(z):\n",
    "    \"\"\"computes the gradient of the sigmoid function\n",
    "    \"\"\"\n",
    "    sigmoid_val = sigmoid(z)\n",
    "    return sigmoid_val * (1 - sigmoid_val)\n",
    "\n",
    "\n",
    "def rescale_image(data, factor):\n",
    "    data_size = int(data.shape[0] ** 0.5)\n",
    "    img = rescale(data.reshape(data_size, data_size), factor, mode='reflect')\n",
    "    x = img.shape[0] ** 2\n",
    "    return img.reshape(x, 1).ravel()\n",
    "\n",
    "\n",
    "def rescale_dataset(dataset, factor=0.75):\n",
    "    rescaled_data = []\n",
    "    for img in dataset:\n",
    "        rescaled_data.append(rescale_image(img, factor))\n",
    "    return np.asarray(rescaled_data)\n",
    "\n",
    "\n",
    "def plot_image(data):\n",
    "    data_size = int(data.shape[0] ** 0.5)\n",
    "    img = data.reshape(data_size, data_size)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_object(obj, file_name):\n",
    "    base_file_name = os.path.basename(file_name)\n",
    "    Path(file_name.replace(base_file_name, '')).mkdir(parents=True, exist_ok=True)\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "\n",
    "def convert_image(data):\n",
    "    k = np.where(data * 255 > 128, 1, 0)\n",
    "    return k\n",
    "\n",
    "\n",
    "def validation_curve(estimator, X, y, param_name, param_range, groups=None,\n",
    "                     cv=None, scoring=None, n_jobs=None, pre_dispatch=\"all\",\n",
    "                     verbose=0, error_score=np.nan):\n",
    "    X, y, groups = indexable(X, y, groups)\n",
    "\n",
    "    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n",
    "    scorer = check_scoring(estimator, scoring=scoring)\n",
    "\n",
    "    parallel = Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch,\n",
    "                        verbose=verbose)\n",
    "    out = parallel(delayed(_fit_and_score)(\n",
    "        clone(estimator), X, y, scorer, train, test, verbose,\n",
    "        parameters={param_name: v}, fit_params=None, return_train_score=True,\n",
    "        error_score=error_score, return_estimator=True, return_times=True)\n",
    "                   # NOTE do not change order of iteration to allow one time cv splitters\n",
    "                   for train, test in cv.split(X, y, groups) for v in param_range)\n",
    "\n",
    "    out = np.asarray(out)\n",
    "    estimators = out[:, 4]\n",
    "    out_scores = np.asarray(out[:, :2])\n",
    "    fit_time = out[:, 2]\n",
    "    score_time = out[:, 3]\n",
    "    n_params = len(param_range)\n",
    "    n_cv_folds = out_scores.shape[0] // n_params\n",
    "    out_scores = out_scores.reshape(n_cv_folds, n_params, 2).transpose((2, 1, 0))\n",
    "\n",
    "    return estimators, np.float64(out_scores[0]), np.float64(out_scores[1]), np.float64(fit_time), \\\n",
    "           np.float64(score_time)\n",
    "\n",
    "\n",
    "def plot_validation_curve(train_scores, test_scores, title, xlabel, ylabel, param_range):\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.ylim(0.0, 1.1)\n",
    "\n",
    "    plt.semilogx(param_range, train_scores, label=\"Training Score\", color=\"blue\", marker=\"o\")\n",
    "    plt.semilogx(param_range, test_scores, label=\"Cross-validation score\", color=\"orange\", marker=\"o\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_time_per_parameter(fit_times, score_times, title, xlabel, ylabel, param_range):\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.semilogx(param_range, fit_times, label=\"Fitting times\", color=\"blue\", marker=\"o\")\n",
    "    plt.semilogx(param_range, score_times, label=\"Scoring times\", color=\"orange\", marker=\"o\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_test_accuracy(x_data, y_data, title, xlabel, ylabel):\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    plt.semilogx(x_data, y_data, label=\"Test set accuracy\", color=\"orange\", marker=\"o\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "def get_classifiers(path):\n",
    "    folders = [f for f in listdir(path)]\n",
    "    classifiers = {}\n",
    "\n",
    "    for folder_name in folders:\n",
    "        folder_path = f\"{path}/{folder_name}\"\n",
    "        if folder_name == 'others':\n",
    "            continue\n",
    "        classifiers[folder_name] = []\n",
    "        for file_name in [f for f in listdir(folder_path)]:\n",
    "            file_path = f\"{folder_path}/{file_name}\"\n",
    "\n",
    "            with open(file_path, 'rb') as output:\n",
    "                classifier = pickle.load(output)\n",
    "                classifiers[folder_name].append(classifier)\n",
    "\n",
    "    return classifiers\n",
    "\n",
    "\n",
    "def save_best_classifiers(classifiers_list, file_name='best_classifiers'):\n",
    "    best = sorted(classifiers_list, reverse=True,\n",
    "                  key=lambda c: (c.params[str(Label.CV)], c.params[str(Label.TRAIN)], c.params[str(Label.TEST)]))[0]\n",
    "\n",
    "    save_object(best, f'{file_name}/{best.name}/{best.variation_param}')\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "@unique\n",
    "class Label(Enum):\n",
    "    TEST = \"accuracy train set\"\n",
    "    CV = \"accuracy cv set\"\n",
    "    TRAIN = \"accuracy test set\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class Classifier(metaclass=ABCMeta):\n",
    "    def __init__(self, name, classifier, X: np.ndarray, y: np.ndarray, variation_param=None):\n",
    "        self.name = name\n",
    "        self.classifier = classifier\n",
    "        self.params = {}\n",
    "        self.variation_param = variation_param\n",
    "\n",
    "        self.X: np.ndarray = X\n",
    "        self.y: np.ndarray = y\n",
    "\n",
    "        self.history = None\n",
    "\n",
    "        self.train_scores = None\n",
    "        self.valid_scores = None\n",
    "\n",
    "    def __train_model(self, x, y):\n",
    "        logger.info(\"Training model...\")\n",
    "        return self.classifier.fit(x, y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        logger.info(\"Predicting...\")\n",
    "        return self.classifier.predict(x)\n",
    "\n",
    "    def error(self, x, y):\n",
    "        logger.info(f\"Calculating error\")\n",
    "        return mean_squared_error(y, self.predict(x)) / 2\n",
    "\n",
    "    def set_new_number_iter(self, iterations):\n",
    "        self.classifier.max_iter = iterations\n",
    "\n",
    "    def train(self, from_previous=False):\n",
    "        if from_previous:\n",
    "            self.classifier.warm_start = from_previous\n",
    "\n",
    "        logger.info(f\"Starting train: {self.name}\")\n",
    "        self.history = self.__train_model(self.X, self.y)\n",
    "\n",
    "    def save_classifier(self, file_name=\"classifier\"):\n",
    "        save_object(self, file_name)\n",
    "\n",
    "    def save_history(self, file_name=\"history\"):\n",
    "        save_object(self.history, file_name)\n",
    "\n",
    "    def generate_report(self, X, y):\n",
    "        return classification_report(y_true=y, y_pred=self.predict(X))\n",
    "\n",
    "    def precision(self, X, y, average=None):\n",
    "        return precision_score(y_true=y, y_pred=self.predict(X), average=average, zero_division=1)\n",
    "\n",
    "    def accuracy(self, X, y, label='accuracy'):\n",
    "        self.params[label] = accuracy_score(y_true=y, y_pred=self.predict(X))\n",
    "        return self.params[label]\n",
    "\n",
    "    def confusion_matrix(self, X, y, label='confusion_matrix'):\n",
    "        self.params[label] = confusion_matrix(y_true=y, y_pred=self.predict(X))\n",
    "        return self.params[label]\n",
    "\n",
    "    def update_params(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            self.params[key] = value\n",
    "        logger.info(f\"Params updated with {kwargs}\")\n",
    "        return self.params\n",
    "\n",
    "    def save_report(self, file_name=\"report.json\"):\n",
    "        with open(file_name, 'w') as file:\n",
    "            file.write(json.dumps(self.generate_report()))\n",
    "        logger.info(f\"Report saved into file: {file_name}\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Classifier : {self.name} ->  {self.params} -> Best value for: {self.variation_param}\\n\"\n",
    "\n",
    "\n",
    "class PolynomialSvm(Classifier):\n",
    "    def __init__(self, classifier, X, y, variation_param):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # self.C = C\n",
    "        # self.degree = degree\n",
    "        self.classifier = classifier\n",
    "        self.variation_param = variation_param\n",
    "        super().__init__(self.__class__.__name__, classifier, self.X, self.y, self.variation_param)\n",
    "\n",
    "    def save_classifier(self, file_name=None):\n",
    "        super().save_classifier(\n",
    "            file_name if file_name is not None else f'classifiers/{self.name}_{self.variation_param}/'\n",
    "                                                    f'{eval(f\"self.classifier.{self.variation_param}\")}.classifier')\n",
    "\n",
    "    def __str__(self):\n",
    "        return super().__str__()  # + f\"C->{self.C}\\tdegree->{self.degree}\\n\"\n",
    "\n",
    "\n",
    "class NeuralNetwork(Classifier):\n",
    "    def __init__(self, X, y, alpha, Lambda, hidden_layer_sizes, iterations, activation, batch_size, solver=\"sgd\",\n",
    "                 variation_param=None, verbose=False):\n",
    "        self.alpha = alpha\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.max_iter = iterations\n",
    "        self.variation_param = variation_param\n",
    "        super().__init__(self.__class__.__name__,\n",
    "                         MLPClassifier(alpha=Lambda, learning_rate_init=alpha, activation=activation,\n",
    "                                       hidden_layer_sizes=self.hidden_layer_sizes, solver=solver,\n",
    "                                       max_iter=iterations, verbose=verbose, n_iter_no_change=10,\n",
    "                                       batch_size=batch_size),\n",
    "                         X, y, self.variation_param)\n",
    "\n",
    "    def save_classifier(self, file_name=None):\n",
    "        super().save_classifier(\n",
    "            file_name if file_name is not None else f'classifiers/{self.name}_alpha_{self.alpha}_'\n",
    "                                                    f'hidden_size_{self.hidden_layer_sizes}_max_iter_{self.max_iter}')\n",
    "\n",
    "    def __str__(self):\n",
    "        return super().__str__() + f\"alpha->{self.alpha}\\thidden_layer_sizes->{self.hidden_layer_sizes}\\tmax_iter->{self.max_iter}\\n\"\n",
    "\n",
    "\n",
    "class LogisticRegression(Classifier):\n",
    "    def __init__(self, classifier, X, y, variation_param):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # self.C = C\n",
    "        # self.max_iter = max_iter\n",
    "        self.variation_param = variation_param\n",
    "        # LogisticRegression_sklearn(C=C, verbose=verbose, max_iter=max_iter, n_jobs=-1),\n",
    "        super().__init__(self.__class__.__name__, classifier, self.X, self.y, self.variation_param)\n",
    "\n",
    "    def save_classifier(self, file_name=None):\n",
    "        super().save_classifier(\n",
    "            file_name if file_name is not None else f'classifiers/{self.name}_{self.variation_param}/'\n",
    "                                                    f'{eval(f\"self.classifier.{self.variation_param}\")}.classifier')  # _C_{self.C}_max_iter_{self.max_iter}')\n",
    "\n",
    "    def __str__(self):\n",
    "        return super().__str__()  # + f\"C->{self.C}\\tmax_iter->{self.max_iter}\\n\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f(filename, x, y):\n",
    "    pixel_size = x.shape[-1]\n",
    "    headlines = ['label']\n",
    "    for i in range(1, pixel_size + 1):\n",
    "        headlines.append(f'pixel{i}')\n",
    "\n",
    "    np.savetxt(filename, np.c_[y, x], delimiter=',', header=','.join(headlines))\n",
    "\n",
    "\n",
    "def set_validation_score_and_curve(classifier, x_train, y_train, x_cv, y_cv, x_test, y_test, parameter,\n",
    "                                   parameter_values, classifier_class):\n",
    "    data_x, data_y = np.concatenate((x_train, x_cv)), np.concatenate((y_train, y_cv))\n",
    "\n",
    "    train_indices = np.full((x_train.shape[0],), -1, dtype=int)\n",
    "    cv_indices = np.full((x_cv.shape[0],), 0, dtype=int)\n",
    "    ps = PredefinedSplit(np.append(train_indices, cv_indices))\n",
    "\n",
    "    estimators_svm, train_scores_svm, valid_scores_svm, fit_times, score_times = validation_curve(\n",
    "        classifier, data_x, data_y.ravel(), parameter, parameter_values, cv=ps, n_jobs=-1)\n",
    "\n",
    "    for i in range(estimators_svm.shape[0]):\n",
    "        classifier = estimators_svm[i]\n",
    "        train_score = train_scores_svm[i]\n",
    "        valid_score = valid_scores_svm[i]\n",
    "        fit_time = fit_times[i]\n",
    "        score_time = score_times[i]\n",
    "\n",
    "        c: Classifier = eval(classifier_class)\n",
    "        c.update_params(train_score=train_score, valid_score=valid_score,\n",
    "                        fit_time=fit_time, score_time=score_time)\n",
    "\n",
    "        c.accuracy(x_test, y_test, \"Test set Accuracy\")\n",
    "        c.confusion_matrix(x_cv, y_cv, \"CV confusion matrix\")\n",
    "        c.confusion_matrix(x_test, y_test, \"Test confusion matrix\")\n",
    "        c.save_classifier()\n",
    "\n",
    "\n",
    "def main():\n",
    "    import google.colab\n",
    "    C = (0.001, 0.002, 0.01, 0.02, 0.1, 0.2, 1, 5, 10, 50, 100, 500, 1000)\n",
    "    degrees = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    alphas = (0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000)\n",
    "    iterations = [200, 500, 1000, 2000]\n",
    "\n",
    "    x_train, y_train = read_file('../dataset/merged_train_set.csv')\n",
    "    x_cv, y_cv = read_file('../dataset/merged_cv_set.csv')\n",
    "    x_test, y_test = read_file('../dataset/merged_test_set.csv')\n",
    "\n",
    "    x_train = x_train / 255\n",
    "    x_cv = x_cv / 255\n",
    "    x_test = x_test / 255\n",
    "\n",
    "    set_validation_score_and_curve(\n",
    "        svm.SVC(kernel='poly', C=C[0], probability=True, degree=degrees[0], verbose=True),\n",
    "        x_train[:100], y_train[:100], x_cv[:100], y_cv[:100], x_test[:100], y_test[:100], \"C\", C,\n",
    "        \"PolynomialSvm(classifier, x_train, y_train, parameter)\")\n",
    "\n",
    "    set_validation_score_and_curve(\n",
    "        LogisticRegressionSKlearn(C=C[0], verbose=True, max_iter=1000, n_jobs=-1),\n",
    "        x_train[:100], y_train[:100], x_cv[:100], y_cv[:100], x_test[:100], y_test[:100], \"C\", C,\n",
    "        \"LogisticRegression(classifier, x_train, y_train, parameter)\")\n",
    "\n",
    "    classifiers = get_classifiers(\"classifiers\")\n",
    "    for classifier_name, classifier_list in classifiers.items():\n",
    "\n",
    "        train_scores = []\n",
    "        valid_scores = []\n",
    "        fit_times = []\n",
    "        score_times = []\n",
    "        tests_accuracy = []\n",
    "\n",
    "        for classifier in classifier_list:\n",
    "            train_scores.append(classifier.params['train_score'])\n",
    "            valid_scores.append(classifier.params['valid_score'])\n",
    "            fit_times.append(classifier.params['fit_time'])\n",
    "            score_times.append(classifier.params['score_time'])\n",
    "            tests_accuracy.append(classifier.params['Test set Accuracy'])\n",
    "\n",
    "        train_scores = np.array(train_scores)\n",
    "        valid_scores = np.array(valid_scores)\n",
    "        fit_times = np.array(fit_times)\n",
    "        score_times = np.array(score_times)\n",
    "        tests_accuracy = np.array(tests_accuracy)\n",
    "\n",
    "        plot_validation_curve(train_scores, valid_scores, f\"Validation Curve with SVM Degree:{degrees[0]}\",\n",
    "                              \"C\", \"Score\", C)\n",
    "\n",
    "        plot_time_per_parameter(fit_times, score_times, \"Time of fitting and scoring proccesses\", \"C\", \"Time (s)\", C)\n",
    "        plot_test_accuracy(C, tests_accuracy, \"Test set accuracy\", \"C\", \"Accuracy\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
